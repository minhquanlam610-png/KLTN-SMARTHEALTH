# ğŸ“˜ Progress Report â€“ XGBoost Model (SisFall)
**Date:** 31/01/2026  
**Student:** Lam Minh Quan  
**Project:** Smart Health Monitoring â€“ Fall Detection (SisFall)

---

## 1. Objective of Todayâ€™s Work
Todayâ€™s objective is to implement and evaluate an **XGBoost (XGBClassifier)** model for fall detection using the **SisFall** dataset, based on the same statistical feature pipeline used in previous experiments.

Main goals:
- Train an XGBoost classifier for fall vs ADL classification
- Handle class imbalance using `scale_pos_weight`
- Evaluate baseline performance
- Analyze the impact of decision threshold on precision/recall trade-off
- Select a final operating threshold for deployment

---

## 2. Dataset & Features
- Input: IMU sliding windows (6 channels: accelerometer + gyroscope)
- Feature set: **30 statistical features/window**
  - mean, std, max, min, energy for each channel
- Train/Test split: 80/20 (stratified)

ğŸ“Œ **Figure placement**
- **Figure 1:** Dataset statistics and windowing summary (if already captured from previous notebook steps)
  - File: `results/sisfall_xgb/xgb_dataset_statistics.png` (optional)

---

## 3. Baseline XGBoost Training (Default Threshold = 0.5)
A baseline XGBClassifier model is trained and evaluated at **threshold = 0.5**.

ğŸ“Œ *Insert figure here*  
**Figure 1.** Baseline classification report of XGBoost at threshold = 0.5.  
File: `results/sisfall_xgb/xgb_baseline_report.png`

ğŸ“Œ *Insert figure here*  
**Figure 2.** Baseline confusion matrix of XGBoost at threshold = 0.5.  
File: `results/sisfall_xgb/xgb_baseline_confusion.png`

**Observation:**
- The baseline achieves high overall accuracy and strong class-wise performance.
- Fall detection recall remains high while keeping false alarms controlled.

---

## 4. Decision Threshold Analysis
To understand the trade-off between missed falls (FN) and false alarms (FP), multiple thresholds are evaluated:
- âˆ’1.0, âˆ’0.5, 0.0 (extreme thresholds)
- 0.35, 0.40, 0.45, 0.50, 0.60 (practical thresholds)

ğŸ“Œ *Insert figure here*  
**Figure 3.** Threshold sweep results for XGBoost, showing classification metrics and confusion matrices across thresholds.  
File: `results/sisfall_xgb/xgb_threshold_sweep.png`

**Notes:**
- Extremely low thresholds may predict nearly all samples as FALL â†’ very high recall but unacceptable false alarms.
- Higher thresholds increase precision but can reduce recall (missed fall events).

---

## 5. Final Threshold Selection (Chosen: 0.45)
For this thesis, **threshold = 0.45** is selected as the final operating point because it provides the **best balance** between precision and recall, with strong overall accuracy.

ğŸ“Œ *Insert figure here*  
**Figure 4.** Final evaluation of XGBoost at threshold = 0.45 (selected threshold).  
File: `results/sisfall_xgb/xgb_threshold_0.45.png`

**Rationale:**
- Balanced FP/FN trade-off
- Stable precision/recall for FALL class
- Suitable for practical deployment where both safety (recall) and usability (precision) are important

---

## 6. Model Saving
The final XGBoost model is saved as a `.joblib` package including:
- trained model
- selected decision threshold (0.45)
- feature description (30 statistical features)
- windowing parameters (WIN/STEP)
- scaler object (if used)

Saved file (local / Kaggle working):
- `sisfall_xgb_final.joblib`

> Note: Large binary model files may be excluded from GitHub to avoid size limits.

---

## 7. Next Steps
- Compare models: Random Forest vs SVM vs XGBoost (same dataset & features)
- Select the best model for final deployment
- Prepare thesis â€œExperimental Resultsâ€ chapter with consolidated tables and discussion
