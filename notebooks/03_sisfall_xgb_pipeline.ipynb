{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7343364,"sourceType":"datasetVersion","datasetId":4263904}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import + C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\nimport os, re\nfrom glob import glob\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:29:29.999992Z","iopub.execute_input":"2026-02-01T02:29:30.000363Z","iopub.status.idle":"2026-02-01T02:29:30.005356Z","shell.execute_reply.started":"2026-02-01T02:29:30.000333Z","shell.execute_reply":"2026-02-01T02:29:30.004317Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Ch·ªët Data_Root + l·∫•y danh s√°ch File\nDATA_ROOT = \"/kaggle/input/sis-fall-original-dataset/SisFall_dataset\"\n\nall_files = glob(DATA_ROOT + \"/SA*/*.txt\")\nprint(\"Total files:\", len(all_files))\nprint(\"Example file:\", all_files[0] if len(all_files) > 0 else \"No files\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:29:30.007311Z","iopub.execute_input":"2026-02-01T02:29:30.007650Z","iopub.status.idle":"2026-02-01T02:29:30.430820Z","shell.execute_reply.started":"2026-02-01T02:29:30.007624Z","shell.execute_reply":"2026-02-01T02:29:30.429908Z"}},"outputs":[{"name":"stdout","text":"Total files: 3537\nExample file: /kaggle/input/sis-fall-original-dataset/SisFall_dataset/SA01/D09_SA01_R02.txt\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# H√†m g√°n nh√£n (FALL/ADL) t·ª´ t√™n File\ndef label_from_filename(path):\n    name = os.path.basename(path).upper()\n    if re.match(r\"F\\d{2}_\", name):\n        return 1  # FALL\n    if re.match(r\"D\\d{2}_\", name):\n        return 0  # ADL\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:29:30.432513Z","iopub.execute_input":"2026-02-01T02:29:30.432905Z","iopub.status.idle":"2026-02-01T02:29:30.437844Z","shell.execute_reply.started":"2026-02-01T02:29:30.432866Z","shell.execute_reply":"2026-02-01T02:29:30.437032Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# H√†m ƒë·ªçc File SisFall\n_num_re = re.compile(r\"[-+]?\\d+(?:[.,]\\d+)?\")\n\ndef load_sisfall_file(path):\n    rows = []\n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n\n            toks = _num_re.findall(line)\n            if len(toks) < 6:\n                continue\n\n            vals = [t.replace(\",\", \".\") for t in toks[:6]]\n            try:\n                rows.append([float(v) for v in vals])\n            except:\n                continue\n\n    if len(rows) == 0:\n        raise ValueError(\"No numeric rows parsed\")\n    return np.asarray(rows, dtype=np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:29:30.439030Z","iopub.execute_input":"2026-02-01T02:29:30.439659Z","iopub.status.idle":"2026-02-01T02:29:30.458436Z","shell.execute_reply.started":"2026-02-01T02:29:30.439632Z","shell.execute_reply":"2026-02-01T02:29:30.457247Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load to√†n b·ªô File -> x_raw, y_raw + th·ªëng k√™\nX_raw, y_raw = [], []\nbad_files = []\n\nfor p in all_files:\n    y = label_from_filename(p)\n    if y is None:\n        continue\n    try:\n        x = load_sisfall_file(p)   # (T, 6)\n        X_raw.append(x)\n        y_raw.append(y)\n    except Exception as e:\n        bad_files.append((p, str(e)))\n\nprint(\"Loaded signals:\", len(X_raw))\nprint(\"Bad files:\", len(bad_files))\nprint(\"FALL count:\", int(np.sum(y_raw)))\nprint(\"ADL count:\", len(y_raw) - int(np.sum(y_raw)))\n\nif len(X_raw) > 0:\n    print(\"Sample shape:\", X_raw[0].shape)\nelse:\n    print(\"No samples loaded yet.\")\n\n# in t·ªëi ƒëa 5 file l·ªói (n·∫øu c√≥)\nfor p, err in bad_files[:5]:\n    print(\"BAD:\", p, \"|\", err)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:29:30.460822Z","iopub.execute_input":"2026-02-01T02:29:30.461280Z","iopub.status.idle":"2026-02-01T02:31:30.786800Z","shell.execute_reply.started":"2026-02-01T02:29:30.461242Z","shell.execute_reply":"2026-02-01T02:31:30.785844Z"}},"outputs":[{"name":"stdout","text":"Loaded signals: 3537\nBad files: 0\nFALL count: 1723\nADL count: 1814\nSample shape: (2400, 6)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Windowing (C·∫Øt c·ª≠a s·ªï)\ndef sliding_window(x, win_size, step):\n    T, C = x.shape\n    if T < win_size:\n        return np.empty((0, win_size, C), dtype=np.float32)\n\n    windows = []\n    for start in range(0, T - win_size + 1, step):\n        windows.append(x[start:start + win_size])\n\n    return np.stack(windows).astype(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:31:30.787976Z","iopub.execute_input":"2026-02-01T02:31:30.788223Z","iopub.status.idle":"2026-02-01T02:31:30.794083Z","shell.execute_reply.started":"2026-02-01T02:31:30.788200Z","shell.execute_reply":"2026-02-01T02:31:30.793261Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# T·∫°o dataset window: X_windows, Y_windows\nWIN = 400\nSTEP = 200\n\nX_windows = []\ny_windows = []\n\nfor x, y in zip(X_raw, y_raw):\n    W = sliding_window(x, WIN, STEP)\n    if len(W) == 0:\n        continue\n    X_windows.append(W)\n    y_windows.append(np.full((W.shape[0],), y, dtype=np.int32))\n\nX_windows = np.concatenate(X_windows, axis=0)\ny_windows = np.concatenate(y_windows, axis=0)\n\nprint(\"X_windows shape:\", X_windows.shape)\nprint(\"y_windows shape:\", y_windows.shape)\nprint(\"Positive rate:\", float(y_windows.mean()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:31:30.795208Z","iopub.execute_input":"2026-02-01T02:31:30.795508Z","iopub.status.idle":"2026-02-01T02:31:31.713692Z","shell.execute_reply.started":"2026-02-01T02:31:30.795483Z","shell.execute_reply":"2026-02-01T02:31:31.712683Z"}},"outputs":[{"name":"stdout","text":"X_windows shape: (56233, 400, 6)\ny_windows shape: (56233,)\nPositive rate: 0.42496398911671085\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Tr√≠ch ƒë·∫∑c tr∆∞ng (Features) cho RandomForest\ndef features_from_window(w):\n    feats = np.concatenate([\n        w.mean(axis=0),\n        w.std(axis=0),\n        w.max(axis=0),\n        w.min(axis=0),\n    ]).astype(np.float32)\n    return feats\n\nX_feat = np.stack([features_from_window(w) for w in X_windows])\nprint(\"X_feat shape:\", X_feat.shape)  # (N, 24)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:31:31.714823Z","iopub.execute_input":"2026-02-01T02:31:31.715336Z","iopub.status.idle":"2026-02-01T02:31:38.836490Z","shell.execute_reply.started":"2026-02-01T02:31:31.715307Z","shell.execute_reply":"2026-02-01T02:31:38.835259Z"}},"outputs":[{"name":"stdout","text":"X_feat shape: (56233, 24)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## üîπ Random Forest (RF)","metadata":{}},{"cell_type":"code","source":"# Train/Test split + Train RandomForest + ƒê√°nh gi√°\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X_feat, y_windows,\n    test_size=0.2,\n    random_state=42,\n    stratify=y_windows\n)\n\nrf = RandomForestClassifier(\n    n_estimators=300,\n    random_state=42,\n    n_jobs=-1,\n    class_weight=\"balanced\"\n)\n\nrf.fit(X_tr, y_tr)\ny_pred = rf.predict(X_te)\n\nprint(classification_report(y_te, y_pred, digits=4))\nprint(\"Confusion matrix:\\n\", confusion_matrix(y_te, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:31:38.837422Z","iopub.execute_input":"2026-02-01T02:31:38.837694Z","iopub.status.idle":"2026-02-01T02:32:19.387172Z","shell.execute_reply.started":"2026-02-01T02:31:38.837672Z","shell.execute_reply":"2026-02-01T02:32:19.386341Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.9056    0.9688    0.9361      6467\n           1     0.9533    0.8634    0.9061      4780\n\n    accuracy                         0.9240     11247\n   macro avg     0.9295    0.9161    0.9211     11247\nweighted avg     0.9259    0.9240    0.9234     11247\n\nConfusion matrix:\n [[6265  202]\n [ 653 4127]]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# L∆∞u Model\nimport joblib\n\njoblib.dump(rf, \"sisfall_random_forest.joblib\")\nprint(\"Saved: sisfall_random_forest.joblib\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:32:19.388283Z","iopub.execute_input":"2026-02-01T02:32:19.388754Z","iopub.status.idle":"2026-02-01T02:32:19.638148Z","shell.execute_reply.started":"2026-02-01T02:32:19.388726Z","shell.execute_reply":"2026-02-01T02:32:19.637253Z"}},"outputs":[{"name":"stdout","text":"Saved: sisfall_random_forest.joblib\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Feature n√¢ng cao\nimport numpy as np\n\ndef advanced_features(w):\n    # w: (WIN,6) -> ax ay az gx gy gz\n    a = w[:, :3]\n    g = w[:, 3:6]\n    amag = np.sqrt((a*a).sum(axis=1))  # (WIN,)\n    gmag = np.sqrt((g*g).sum(axis=1))\n\n    feats = []\n\n    # stats 6 k√™nh g·ªëc\n    feats += list(w.mean(axis=0))\n    feats += list(w.std(axis=0))\n    feats += list(w.max(axis=0))\n    feats += list(w.min(axis=0))\n\n    # stats magnitude\n    feats += [amag.mean(), amag.std(), amag.max(), amag.min()]\n    feats += [gmag.mean(), gmag.std(), gmag.max(), gmag.min()]\n\n    # energy (mean squared)\n    feats += [(w*w).mean()]\n    feats += [(amag*amag).mean()]\n    feats += [(gmag*gmag).mean()]\n\n    return np.array(feats, dtype=np.float32)\n\nX_feat2 = np.stack([advanced_features(w) for w in X_windows])\nprint(\"X_feat2 shape:\", X_feat2.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:32:19.640889Z","iopub.execute_input":"2026-02-01T02:32:19.641190Z","iopub.status.idle":"2026-02-01T02:32:37.682605Z","shell.execute_reply.started":"2026-02-01T02:32:19.641164Z","shell.execute_reply":"2026-02-01T02:32:37.681427Z"}},"outputs":[{"name":"stdout","text":"X_feat2 shape: (56233, 35)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Train l·∫°i RF tr√™n feature m·ªõi + th·ª≠ threshold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X_feat2, y_windows, test_size=0.2, random_state=42, stratify=y_windows\n)\n\nrf2 = RandomForestClassifier(\n    n_estimators=600,\n    random_state=42,\n    n_jobs=-1,\n    class_weight=\"balanced_subsample\",\n    max_depth=None\n)\n\nrf2.fit(X_tr, y_tr)\n\nproba = rf2.predict_proba(X_te)[:, 1]\n\nfor thr in [0.5, 0.45, 0.4, 0.35]:\n    y_pred = (proba >= thr).astype(int)\n    print(\"\\n==== threshold =\", thr, \"====\")\n    print(classification_report(y_te, y_pred, digits=4))\n    print(\"Confusion matrix:\\n\", confusion_matrix(y_te, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:32:37.683898Z","iopub.execute_input":"2026-02-01T02:32:37.684353Z","iopub.status.idle":"2026-02-01T02:34:24.182499Z","shell.execute_reply.started":"2026-02-01T02:32:37.684314Z","shell.execute_reply":"2026-02-01T02:34:24.181567Z"}},"outputs":[{"name":"stdout","text":"\n==== threshold = 0.5 ====\n              precision    recall  f1-score   support\n\n           0     0.9110    0.9702    0.9396      6467\n           1     0.9557    0.8718    0.9118      4780\n\n    accuracy                         0.9283     11247\n   macro avg     0.9334    0.9210    0.9257     11247\nweighted avg     0.9300    0.9283    0.9278     11247\n\nConfusion matrix:\n [[6274  193]\n [ 613 4167]]\n\n==== threshold = 0.45 ====\n              precision    recall  f1-score   support\n\n           0     0.9293    0.9592    0.9440      6467\n           1     0.9423    0.9013    0.9213      4780\n\n    accuracy                         0.9346     11247\n   macro avg     0.9358    0.9302    0.9326     11247\nweighted avg     0.9348    0.9346    0.9344     11247\n\nConfusion matrix:\n [[6203  264]\n [ 472 4308]]\n\n==== threshold = 0.4 ====\n              precision    recall  f1-score   support\n\n           0     0.9432    0.9417    0.9424      6467\n           1     0.9213    0.9232    0.9223      4780\n\n    accuracy                         0.9338     11247\n   macro avg     0.9322    0.9325    0.9323     11247\nweighted avg     0.9339    0.9338    0.9339     11247\n\nConfusion matrix:\n [[6090  377]\n [ 367 4413]]\n\n==== threshold = 0.35 ====\n              precision    recall  f1-score   support\n\n           0     0.9576    0.9184    0.9376      6467\n           1     0.8953    0.9450    0.9195      4780\n\n    accuracy                         0.9297     11247\n   macro avg     0.9265    0.9317    0.9285     11247\nweighted avg     0.9311    0.9297    0.9299     11247\n\nConfusion matrix:\n [[5939  528]\n [ 263 4517]]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import joblib\n\njoblib.dump(\n    {\n        \"model\": rf2,\n        \"threshold\": 0.4,\n        \"features\": \"advanced_features\",\n        \"win\": WIN,\n        \"step\": STEP\n    },\n    \"sisfall_rf_final.joblib\"\n)\n\nprint(\"Saved final model: sisfall_rf_final.joblib\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:34:24.183677Z","iopub.execute_input":"2026-02-01T02:34:24.184324Z","iopub.status.idle":"2026-02-01T02:34:24.664543Z","shell.execute_reply.started":"2026-02-01T02:34:24.184280Z","shell.execute_reply":"2026-02-01T02:34:24.663597Z"}},"outputs":[{"name":"stdout","text":"Saved final model: sisfall_rf_final.joblib\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## üîπ Support Vector Machine (SVM)","metadata":{}},{"cell_type":"code","source":"# Feature extraction\nimport numpy as np\n\ndef extract_features(window):\n    \"\"\"\n    window: (WIN, 6)\n    return: (30,) feature vector\n    \"\"\"\n    feats = []\n    for ch in range(window.shape[1]):\n        sig = window[:, ch]\n        feats.extend([\n            np.mean(sig),\n            np.std(sig),\n            np.max(sig),\n            np.min(sig),\n            np.mean(sig ** 2)  # energy\n        ])\n    return np.array(feats, dtype=np.float32)\n\n# Tr√≠ch feature cho to√†n b·ªô dataset\nX_svm = np.array([extract_features(w) for w in X_windows])\ny_svm = y_windows.copy()\n\nprint(\"X_svm shape:\", X_svm.shape)\nprint(\"y_svm shape:\", y_svm.shape)\nprint(\"Sample feature vector length:\", X_svm.shape[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:34:24.665844Z","iopub.execute_input":"2026-02-01T02:34:24.666870Z","iopub.status.idle":"2026-02-01T02:34:46.090886Z","shell.execute_reply.started":"2026-02-01T02:34:24.666815Z","shell.execute_reply":"2026-02-01T02:34:46.089947Z"}},"outputs":[{"name":"stdout","text":"X_svm shape: (56233, 30)\ny_svm shape: (56233,)\nSample feature vector length: 30\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Scale + split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chia train / test\nX_train, X_test, y_train, y_test = train_test_split(\n    X_svm, y_svm,\n    test_size=0.2,\n    random_state=42,\n    stratify=y_svm\n)\n\n# Chu·∫©n h√≥a feature\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"Train shape:\", X_train_scaled.shape)\nprint(\"Test shape :\", X_test_scaled.shape)\nprint(\"Mean (train, first feature):\", X_train_scaled[:, 0].mean())\nprint(\"Std  (train, first feature):\", X_train_scaled[:, 0].std())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:34:46.092201Z","iopub.execute_input":"2026-02-01T02:34:46.092598Z","iopub.status.idle":"2026-02-01T02:34:46.156447Z","shell.execute_reply.started":"2026-02-01T02:34:46.092561Z","shell.execute_reply":"2026-02-01T02:34:46.155520Z"}},"outputs":[{"name":"stdout","text":"Train shape: (44986, 30)\nTest shape : (11247, 30)\nMean (train, first feature): 0.0\nStd  (train, first feature): 1.0\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Train + Evaluate\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport time\n\n# ===== 1) SVM Linear (NHANH) =====\nt0 = time.time()\nsvm_linear = LinearSVC(\n    C=1.0,\n    class_weight=\"balanced\",\n    max_iter=20000,\n    random_state=42\n)\nsvm_linear.fit(X_train_scaled, y_train)\nt1 = time.time()\n\npred_linear = svm_linear.predict(X_test_scaled)\n\nprint(\"=== SVM Linear ===\")\nprint(f\"Train time: {t1 - t0:.2f}s\")\nprint(classification_report(y_test, pred_linear, digits=4))\nprint(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred_linear))\n\n# ===== 2) SVM RBF (M·∫†NH nh∆∞ng c√≥ th·ªÉ CH·∫¨M) =====\n# N·∫øu b·∫°n mu·ªën th·ª≠ lu√¥n, b·ªè comment 2 d√≤ng d∆∞·ªõi:\n# run_rbf = True\nrun_rbf = False\n\nif run_rbf:\n    t0 = time.time()\n    svm_rbf = SVC(\n        kernel=\"rbf\",\n        C=3.0,\n        gamma=\"scale\",\n        class_weight=\"balanced\"\n    )\n    svm_rbf.fit(X_train_scaled, y_train)\n    t1 = time.time()\n\n    pred_rbf = svm_rbf.predict(X_test_scaled)\n\n    print(\"\\n=== SVM RBF ===\")\n    print(f\"Train time: {t1 - t0:.2f}s\")\n    print(classification_report(y_test, pred_rbf, digits=4))\n    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred_rbf))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:34:46.157934Z","iopub.execute_input":"2026-02-01T02:34:46.158334Z","iopub.status.idle":"2026-02-01T02:34:46.891288Z","shell.execute_reply.started":"2026-02-01T02:34:46.158296Z","shell.execute_reply":"2026-02-01T02:34:46.890330Z"}},"outputs":[{"name":"stdout","text":"=== SVM Linear ===\nTrain time: 0.71s\n              precision    recall  f1-score   support\n\n           0     0.7835    0.8967    0.8363      6467\n           1     0.8263    0.6649    0.7368      4780\n\n    accuracy                         0.7982     11247\n   macro avg     0.8049    0.7808    0.7866     11247\nweighted avg     0.8017    0.7982    0.7940     11247\n\nConfusion matrix:\n [[5799  668]\n [1602 3178]]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":" # SVM Threshold Tuning \nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# L·∫•y decision score\nscores = svm_linear.decision_function(X_test_scaled)\n\nthresholds = [-1.0, -0.5, 0.0, 0.5, 1.0]\n\nfor th in thresholds:\n    y_pred_th = (scores > th).astype(int)\n    print(f\"\\n=== Threshold = {th} ===\")\n    print(classification_report(y_test, y_pred_th, digits=4))\n    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_th))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:34:46.892632Z","iopub.execute_input":"2026-02-01T02:34:46.893025Z","iopub.status.idle":"2026-02-01T02:34:46.957354Z","shell.execute_reply.started":"2026-02-01T02:34:46.892985Z","shell.execute_reply":"2026-02-01T02:34:46.956475Z"}},"outputs":[{"name":"stdout","text":"\n=== Threshold = -1.0 ===\n              precision    recall  f1-score   support\n\n           0     0.9379    0.0724    0.1344      6467\n           1     0.4418    0.9935    0.6117      4780\n\n    accuracy                         0.4639     11247\n   macro avg     0.6899    0.5329    0.3730     11247\nweighted avg     0.7271    0.4639    0.3372     11247\n\nConfusion matrix:\n [[ 468 5999]\n [  31 4749]]\n\n=== Threshold = -0.5 ===\n              precision    recall  f1-score   support\n\n           0     0.9312    0.3620    0.5213      6467\n           1     0.5275    0.9638    0.6819      4780\n\n    accuracy                         0.6178     11247\n   macro avg     0.7294    0.6629    0.6016     11247\nweighted avg     0.7596    0.6178    0.5896     11247\n\nConfusion matrix:\n [[2341 4126]\n [ 173 4607]]\n\n=== Threshold = 0.0 ===\n              precision    recall  f1-score   support\n\n           0     0.7835    0.8967    0.8363      6467\n           1     0.8263    0.6649    0.7368      4780\n\n    accuracy                         0.7982     11247\n   macro avg     0.8049    0.7808    0.7866     11247\nweighted avg     0.8017    0.7982    0.7940     11247\n\nConfusion matrix:\n [[5799  668]\n [1602 3178]]\n\n=== Threshold = 0.5 ===\n              precision    recall  f1-score   support\n\n           0     0.7592    0.9497    0.8439      6467\n           1     0.8971    0.5925    0.7136      4780\n\n    accuracy                         0.7979     11247\n   macro avg     0.8281    0.7711    0.7787     11247\nweighted avg     0.8178    0.7979    0.7885     11247\n\nConfusion matrix:\n [[6142  325]\n [1948 2832]]\n\n=== Threshold = 1.0 ===\n              precision    recall  f1-score   support\n\n           0     0.6129    0.9946    0.7584      6467\n           1     0.9535    0.1502    0.2595      4780\n\n    accuracy                         0.6357     11247\n   macro avg     0.7832    0.5724    0.5090     11247\nweighted avg     0.7577    0.6357    0.5464     11247\n\nConfusion matrix:\n [[6432   35]\n [4062  718]]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# SVM Save\nimport joblib\n\n# 1) Baseline config\njoblib.dump(\n    {\n        \"model\": svm_linear,\n        \"scaler\": scaler,\n        \"features\": \"statistical_30 (mean,std,max,min,energy) on 6 channels\",\n        \"threshold\": 0.0,\n        \"win\": WIN,\n        \"step\": STEP\n    },\n    \"svm_linear_baseline.joblib\"\n)\n\n# 2) High-recall config (∆∞u ti√™n b·∫Øt ng√£)\njoblib.dump(\n    {\n        \"model\": svm_linear,\n        \"scaler\": scaler,\n        \"features\": \"statistical_30 (mean,std,max,min,energy) on 6 channels\",\n        \"threshold\": -0.5,\n        \"win\": WIN,\n        \"step\": STEP\n    },\n    \"svm_linear_highrecall.joblib\"\n)\n\nprint(\"Saved:\")\nprint(\"- svm_linear_baseline.joblib\")\nprint(\"- svm_linear_highrecall.joblib\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:34:46.958715Z","iopub.execute_input":"2026-02-01T02:34:46.959097Z","iopub.status.idle":"2026-02-01T02:34:46.969749Z","shell.execute_reply.started":"2026-02-01T02:34:46.959059Z","shell.execute_reply":"2026-02-01T02:34:46.968635Z"}},"outputs":[{"name":"stdout","text":"Saved:\n- svm_linear_baseline.joblib\n- svm_linear_highrecall.joblib\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## üîπ XGBoost","metadata":{}},{"cell_type":"code","source":"# Train + Evaluate baseline\nimport numpy as np\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# XGBoost kh√¥ng b·∫Øt bu·ªôc scale, nh∆∞ng ƒë·ªÉ FAIR v·ªõi SVM\n# ta s·∫Ω d√πng ƒë√∫ng d·ªØ li·ªáu ƒë√£ scale tr∆∞·ªõc ƒë√≥: X_train_scaled, X_test_scaled\nXtr = X_train_scaled\nXte = X_test_scaled\nytr = y_train\nyte = y_test\n\n# T√≠nh imbalance ratio ƒë·ªÉ set scale_pos_weight (gi√∫p c√¢n b·∫±ng class)\nneg = np.sum(ytr == 0)\npos = np.sum(ytr == 1)\nscale_pos_weight = neg / pos\nprint(\"Train class count -> neg:\", neg, \"pos:\", pos, \"scale_pos_weight:\", scale_pos_weight)\n\nxgb = XGBClassifier(\n    n_estimators=400,\n    max_depth=6,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_lambda=1.0,\n    objective=\"binary:logistic\",\n    eval_metric=\"logloss\",\n    tree_method=\"hist\",     # ch·∫°y nhanh tr√™n CPU (Kaggle)\n    random_state=42,\n    n_jobs=-1,\n    scale_pos_weight=scale_pos_weight\n)\n\nxgb.fit(Xtr, ytr)\n\n# Predict\nproba = xgb.predict_proba(Xte)[:, 1]\npred = (proba >= 0.5).astype(int)\n\nprint(\"=== XGBoost Baseline (threshold=0.5) ===\")\nprint(classification_report(yte, pred, digits=4))\nprint(\"Confusion matrix:\")\nprint(confusion_matrix(yte, pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:34:46.970987Z","iopub.execute_input":"2026-02-01T02:34:46.971348Z","iopub.status.idle":"2026-02-01T02:34:50.667554Z","shell.execute_reply.started":"2026-02-01T02:34:46.971315Z","shell.execute_reply":"2026-02-01T02:34:50.666651Z"}},"outputs":[{"name":"stdout","text":"Train class count -> neg: 25869 pos: 19117 scale_pos_weight: 1.35319349270283\n=== XGBoost Baseline (threshold=0.5) ===\n              precision    recall  f1-score   support\n\n           0     0.9193    0.9457    0.9323      6467\n           1     0.9236    0.8877    0.9053      4780\n\n    accuracy                         0.9210     11247\n   macro avg     0.9214    0.9167    0.9188     11247\nweighted avg     0.9211    0.9210    0.9208     11247\n\nConfusion matrix:\n[[6116  351]\n [ 537 4243]]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# XGBoost Threshold Analysis\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nthresholds = [-1.0, -0.5, 0.0, 0.35, 0.4, 0.45, 0.5, 0.6]\n\n# proba ƒë√£ c√≥ t·ª´ cell tr∆∞·ªõc: proba = xgb.predict_proba(Xte)[:, 1]\n# n·∫øu b·∫°n restart kernel, h√£y ch·∫°y l·∫°i cell baseline tr∆∞·ªõc ƒë·ªÉ c√≥ bi·∫øn proba\n\nfor t in thresholds:\n    print(f\"\\n===== threshold = {t} =====\")\n    pred_t = (proba >= t).astype(int)\n    print(classification_report(yte, pred_t, digits=4))\n    print(\"Confusion matrix:\")\n    print(confusion_matrix(yte, pred_t))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:34:50.668656Z","iopub.execute_input":"2026-02-01T02:34:50.669003Z","iopub.status.idle":"2026-02-01T02:34:50.772659Z","shell.execute_reply.started":"2026-02-01T02:34:50.668976Z","shell.execute_reply":"2026-02-01T02:34:50.771548Z"}},"outputs":[{"name":"stdout","text":"\n===== threshold = -1.0 =====\n              precision    recall  f1-score   support\n\n           0     0.0000    0.0000    0.0000      6467\n           1     0.4250    1.0000    0.5965      4780\n\n    accuracy                         0.4250     11247\n   macro avg     0.2125    0.5000    0.2982     11247\nweighted avg     0.1806    0.4250    0.2535     11247\n\nConfusion matrix:\n[[   0 6467]\n [   0 4780]]\n\n===== threshold = -0.5 =====\n              precision    recall  f1-score   support\n\n           0     0.0000    0.0000    0.0000      6467\n           1     0.4250    1.0000    0.5965      4780\n\n    accuracy                         0.4250     11247\n   macro avg     0.2125    0.5000    0.2982     11247\nweighted avg     0.1806    0.4250    0.2535     11247\n\nConfusion matrix:\n[[   0 6467]\n [   0 4780]]\n\n===== threshold = 0.0 =====\n              precision    recall  f1-score   support\n\n           0     0.0000    0.0000    0.0000      6467\n           1     0.4250    1.0000    0.5965      4780\n\n    accuracy                         0.4250     11247\n   macro avg     0.2125    0.5000    0.2982     11247\nweighted avg     0.1806    0.4250    0.2535     11247\n\nConfusion matrix:\n[[   0 6467]\n [   0 4780]]\n\n===== threshold = 0.35 =====\n              precision    recall  f1-score   support\n\n           0     0.9496    0.8916    0.9197      6467\n           1     0.8645    0.9360    0.8988      4780\n\n    accuracy                         0.9105     11247\n   macro avg     0.9071    0.9138    0.9093     11247\nweighted avg     0.9135    0.9105    0.9108     11247\n\nConfusion matrix:\n[[5766  701]\n [ 306 4474]]\n\n===== threshold = 0.4 =====\n              precision    recall  f1-score   support\n\n           0     0.9400    0.9140    0.9269      6467\n           1     0.8879    0.9211    0.9042      4780\n\n    accuracy                         0.9170     11247\n   macro avg     0.9140    0.9176    0.9155     11247\nweighted avg     0.9179    0.9170    0.9172     11247\n\nConfusion matrix:\n[[5911  556]\n [ 377 4403]]\n\n===== threshold = 0.45 =====\n              precision    recall  f1-score   support\n\n           0     0.9320    0.9323    0.9321      6467\n           1     0.9083    0.9079    0.9081      4780\n\n    accuracy                         0.9219     11247\n   macro avg     0.9202    0.9201    0.9201     11247\nweighted avg     0.9219    0.9219    0.9219     11247\n\nConfusion matrix:\n[[6029  438]\n [ 440 4340]]\n\n===== threshold = 0.5 =====\n              precision    recall  f1-score   support\n\n           0     0.9193    0.9457    0.9323      6467\n           1     0.9236    0.8877    0.9053      4780\n\n    accuracy                         0.9210     11247\n   macro avg     0.9214    0.9167    0.9188     11247\nweighted avg     0.9211    0.9210    0.9208     11247\n\nConfusion matrix:\n[[6116  351]\n [ 537 4243]]\n\n===== threshold = 0.6 =====\n              precision    recall  f1-score   support\n\n           0     0.8996    0.9647    0.9311      6467\n           1     0.9471    0.8544    0.8984      4780\n\n    accuracy                         0.9178     11247\n   macro avg     0.9234    0.9096    0.9147     11247\nweighted avg     0.9198    0.9178    0.9172     11247\n\nConfusion matrix:\n[[6239  228]\n [ 696 4084]]\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Save Model XGBoost (Threshold = 0.45)\nimport joblib\nimport os\nimport numpy as np\n\n# ====== b·∫°n CH·ªà c·∫ßn ƒë·∫£m b·∫£o 2 bi·∫øn n√†y ƒëang t·ªìn t·∫°i trong notebook ======\n# 1) xgb_clf  : model XGBClassifier ƒë√£ fit\n# 2) scaler   : StandardScaler ƒë√£ fit (n·∫øu b·∫°n c√≥ d√πng chu·∫©n h√≥a)\n# N·∫øu b·∫°n KH√îNG d√πng scaler cho XGB, m√¨nh c√≥ x·ª≠ l√Ω ·ªü d∆∞·ªõi.\n\nTHRESHOLD_FINAL = 0.45\n\n# N·∫øu b·∫°n c√≥ scaler -> OK.\n# N·∫øu kh√¥ng c√≥ scaler, ƒëo·∫°n n√†y s·∫Ω t·ª± set = None ƒë·ªÉ v·∫´n l∆∞u ƒë∆∞·ª£c.\nscaler_to_save = None\ntry:\n    scaler_to_save = scaler\nexcept NameError:\n    scaler_to_save = None\n\n# L∆∞u v√†o /kaggle/working ƒë·ªÉ t·∫£i v·ªÅ ƒë∆∞·ª£c\nOUT_PATH = \"/kaggle/working/sisfall_xgb_final.joblib\"\n\npayload = {\n    \"model\": xgb_clf,                 # ƒë·ªïi t√™n n·∫øu model b·∫°n ƒë·∫∑t kh√°c\n    \"threshold\": THRESHOLD_FINAL,\n    \"features\": \"stat_30_features\",   # b·∫°n ƒëang d√πng 30 features nh∆∞ SVM\n    \"win\": WIN,\n    \"step\": STEP,\n    \"scaler\": scaler_to_save\n}\n\njoblib.dump(payload, OUT_PATH)\n\nprint(\"‚úÖ Saved:\", OUT_PATH)\nprint(\"Contains keys:\", list(payload.keys()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T02:34:50.774150Z","iopub.execute_input":"2026-02-01T02:34:50.774611Z","iopub.status.idle":"2026-02-01T02:34:50.785773Z","shell.execute_reply.started":"2026-02-01T02:34:50.774581Z","shell.execute_reply":"2026-02-01T02:34:50.784378Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2312173134.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m payload = {\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxgb_clf\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;31m# ƒë·ªïi t√™n n·∫øu model b·∫°n ƒë·∫∑t kh√°c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;34m\"threshold\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTHRESHOLD_FINAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;34m\"features\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"stat_30_features\"\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# b·∫°n ƒëang d√πng 30 features nh∆∞ SVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'xgb_clf' is not defined"],"ename":"NameError","evalue":"name 'xgb_clf' is not defined","output_type":"error"}],"execution_count":22}]}